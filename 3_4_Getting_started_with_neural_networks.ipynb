{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "3-4. Getting started with neural networks.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keywoong/deeplearning_with_python/blob/main/3_4_Getting_started_with_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuN9UgRZczCc"
      },
      "source": [
        "# 3.6 Predicting house prices : a regression example\n",
        "#### 회귀 문제는 라벨로 구별하는 것이 아니라 값을 예측하는 문제이다. regression과 달리 logistic regression은 분류문제에 사용되는 알고리즘이다.\n",
        "## 3.6.1 The Boston Housing Price dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQvAtz07czCl",
        "outputId": "a0035bf5-d657-4c3f-86f3-2db753d624c8"
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
        "\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(404, 13)\n",
            "(102, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZyFkrj4czCn"
      },
      "source": [
        "#### 위의 결과처럼, training 데이터셋은 404개이고, test 데이터셋은 102개이다. 그리고 그들을 구성하는 특징의 개수는 총 13개이다. (특징은 범죄율, 평당 가격, 고속도로 접근 가능성 등이 있겠다.)\n",
        "#### target값은 집값이다. (단위는 천달러)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgwAD2YYczCo",
        "outputId": "abfd7a26-4433-414b-b800-de2241bc50d4"
      },
      "source": [
        "print(train_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[15.2 42.3 50.  21.1 17.7 18.5 11.3 15.6 15.6 14.4 12.1 17.9 23.1 19.9\n",
            " 15.7  8.8 50.  22.5 24.1 27.5 10.9 30.8 32.9 24.  18.5 13.3 22.9 34.7\n",
            " 16.6 17.5 22.3 16.1 14.9 23.1 34.9 25.  13.9 13.1 20.4 20.  15.2 24.7\n",
            " 22.2 16.7 12.7 15.6 18.4 21.  30.1 15.1 18.7  9.6 31.5 24.8 19.1 22.\n",
            " 14.5 11.  32.  29.4 20.3 24.4 14.6 19.5 14.1 14.3 15.6 10.5  6.3 19.3\n",
            " 19.3 13.4 36.4 17.8 13.5 16.5  8.3 14.3 16.  13.4 28.6 43.5 20.2 22.\n",
            " 23.  20.7 12.5 48.5 14.6 13.4 23.7 50.  21.7 39.8 38.7 22.2 34.9 22.5\n",
            " 31.1 28.7 46.  41.7 21.  26.6 15.  24.4 13.3 21.2 11.7 21.7 19.4 50.\n",
            " 22.8 19.7 24.7 36.2 14.2 18.9 18.3 20.6 24.6 18.2  8.7 44.  10.4 13.2\n",
            " 21.2 37.  30.7 22.9 20.  19.3 31.7 32.  23.1 18.8 10.9 50.  19.6  5.\n",
            " 14.4 19.8 13.8 19.6 23.9 24.5 25.  19.9 17.2 24.6 13.5 26.6 21.4 11.9\n",
            " 22.6 19.6  8.5 23.7 23.1 22.4 20.5 23.6 18.4 35.2 23.1 27.9 20.6 23.7\n",
            " 28.  13.6 27.1 23.6 20.6 18.2 21.7 17.1  8.4 25.3 13.8 22.2 18.4 20.7\n",
            " 31.6 30.5 20.3  8.8 19.2 19.4 23.1 23.  14.8 48.8 22.6 33.4 21.1 13.6\n",
            " 32.2 13.1 23.4 18.9 23.9 11.8 23.3 22.8 19.6 16.7 13.4 22.2 20.4 21.8\n",
            " 26.4 14.9 24.1 23.8 12.3 29.1 21.  19.5 23.3 23.8 17.8 11.5 21.7 19.9\n",
            " 25.  33.4 28.5 21.4 24.3 27.5 33.1 16.2 23.3 48.3 22.9 22.8 13.1 12.7\n",
            " 22.6 15.  15.3 10.5 24.  18.5 21.7 19.5 33.2 23.2  5.  19.1 12.7 22.3\n",
            " 10.2 13.9 16.3 17.  20.1 29.9 17.2 37.3 45.4 17.8 23.2 29.  22.  18.\n",
            " 17.4 34.6 20.1 25.  15.6 24.8 28.2 21.2 21.4 23.8 31.  26.2 17.4 37.9\n",
            " 17.5 20.   8.3 23.9  8.4 13.8  7.2 11.7 17.1 21.6 50.  16.1 20.4 20.6\n",
            " 21.4 20.6 36.5  8.5 24.8 10.8 21.9 17.3 18.9 36.2 14.9 18.2 33.3 21.8\n",
            " 19.7 31.6 24.8 19.4 22.8  7.5 44.8 16.8 18.7 50.  50.  19.5 20.1 50.\n",
            " 17.2 20.8 19.3 41.3 20.4 20.5 13.8 16.5 23.9 20.6 31.5 23.3 16.8 14.\n",
            " 33.8 36.1 12.8 18.3 18.7 19.1 29.  30.1 50.  50.  22.  11.9 37.6 50.\n",
            " 22.7 20.8 23.5 27.9 50.  19.3 23.9 22.6 15.2 21.7 19.2 43.8 20.3 33.2\n",
            " 19.9 22.5 32.7 22.  17.1 19.  15.  16.1 25.1 23.7 28.7 37.2 22.6 16.4\n",
            " 25.  29.8 22.1 17.4 18.1 30.3 17.5 24.7 12.6 26.5 28.7 13.3 10.4 24.4\n",
            " 23.  20.  17.8  7.  11.8 24.4 13.8 19.4 25.2 19.4 19.4 29.1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUAIx1kwczCo"
      },
      "source": [
        "## 3.6.2 Preparing the data\n",
        "#### 데이터들은 10000달러에서 50000달러 사이에 분포하는데 이는 widespreading하다. 우리는 이 데이터들을 정규화를 시켜서 0을 기준으로 분포하도록 만들어 줘야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUCqrPK6czCp",
        "outputId": "9d7120b9-bd7e-4ab6-a88a-06a789b7f0d3"
      },
      "source": [
        "# Normaliziing the data\n",
        "mean = train_data.mean(axis = 0)\n",
        "train_data -=mean\n",
        "std = train_data.std(axis = 0)\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std\n",
        "\n",
        "print(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.27224633 -0.48361547 -0.43576161 ...  1.14850044  0.44807713\n",
            "   0.8252202 ]\n",
            " [-0.40342651  2.99178419 -1.33391162 ... -1.71818909  0.43190599\n",
            "  -1.32920239]\n",
            " [ 0.1249402  -0.48361547  1.0283258  ...  0.78447637  0.22061726\n",
            "  -1.30850006]\n",
            " ...\n",
            " [-0.40202987  0.99079651 -0.7415148  ... -0.71712291  0.07943894\n",
            "  -0.67776904]\n",
            " [-0.17292018 -0.48361547  1.24588095 ... -1.71818909 -0.98764362\n",
            "   0.42083466]\n",
            " [-0.40422614  2.04394792 -1.20161456 ... -1.30866202  0.23317118\n",
            "  -1.15392266]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vobwv5LkczCq"
      },
      "source": [
        "#### 위의 과정처럼 정규화 과정을 진행한다.\n",
        "#### 정규화 : $ Z = (X-m) / \\sigma $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaC8slRnczCq"
      },
      "source": [
        "## 3.6.3 Building your network\n",
        "#### 404개의 적은 데이터만 사용이 가능하므로, 64개의 unit을 가지는 2개의 은닉층으로 구성된 매우 작은 네트워크를 만들 것이다.\n",
        "#### 일반적으로 데이터의 개수가 적으면 오버피팅이 발생할 가능성이 높아지기 때문에, 네트워크를 작게 만드는 것이 하나의 해결방법이 될 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSLPSp-gczCr"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "def build_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(64, activation = 'relu', input_shape = (train_data.shape[1],)))\n",
        "    model.add(layers.Dense(64, activation = 'relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Lyf0uFczCr"
      },
      "source": [
        "#### 이 모델의 마지막 층은 1개의 unit만이 있고, 활성화 함수도 없는 것으로 보아 스칼라를 출력하는 선형함수임을 알 수 있다.\n",
        "#### 이 선형함수를 통해 원하는 값을 예측할 수 있는 것이다 (regression)\n",
        "#### 만약 sigmoid함수를 사용했다면, 0과 1 사이의 값으로 출력하므로 순수 원하는 값을 얻을 수 없다.\n",
        "#### 이 네트워크에서는 `mse` 손실함수를 사용했다. `(mean squared error)` 최소제곱법으로, 회귀 문제에 자주 사용된다. (2차 norm)\n",
        "#### 또한 `mae`를 monitoring metric으로 선정했다. `(Mean Absolute Error)` 이것은 예측값과 실제값의 절대적인 차이값이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSiVmwvcczCs"
      },
      "source": [
        "## 3.6.4 Validating your approach using K-fold validation\n",
        "#### 이진분류나 다중분류때 했던 것처럼 training set으로 매개변수를 수정해나가고, 마지막으로 validation data로 점검을 하려고 한다.\n",
        "#### 그런데 이 dataset은 자료의 수가 너무 적기 때문에 (404개, 102개) 그렇게 하기가 힘들다.\n",
        "#### 이러한 경우에 K-fold를 사용한다.\n",
        "#### 사용 가능한 데이터를 K 개의 파티션으로 분할하고, K 개의 동일한 모델을 인스턴스화하고, 나머지 파티션에서 평가하는 동안 K-1 파티션에서 각 모델을 훈련하는 것으로 구성됩니다.\n",
        "#### 사용 된 모델의 검증 점수는 획득 한 K 검증 점수의 평균입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B94-lnAkczCs",
        "outputId": "32ef54ad-1ddf-40ec-c4d2-762b383a8df9"
      },
      "source": [
        "# K-fold Validation\n",
        "import numpy as np\n",
        "\n",
        "k = 4\n",
        "num_val_samples = int(len(train_data) / k) # k개의 파티션으로 분할\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    val_data = train_data[i*num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples : (i + 1) * num_val_samples]\n",
        "    \n",
        "    partial_train_data = np.concatenate(\n",
        "    [train_data[:i * num_val_samples],\n",
        "    train_data[(i + 1) * num_val_samples :]],\n",
        "    axis = 0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "    [train_targets[:i * num_val_samples],\n",
        "    train_targets[(i + 1) * num_val_samples :]],\n",
        "    axis = 0)\n",
        "    \n",
        "    model = build_model()\n",
        "    model.fit(partial_train_data, partial_train_targets, epochs = num_epochs, batch_size = 1, verbose = 0)\n",
        "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose = 0)\n",
        "    all_scores.append(val_mae)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6cn6oOoczCt",
        "outputId": "6ab360d7-58c0-4ec8-f072-778069f74100"
      },
      "source": [
        "print(all_scores)\n",
        "print('----------------')\n",
        "print(np.mean(all_scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.1120738983154297, 2.3811111450195312, 2.6254844665527344, 2.4106967449188232]\n",
            "----------------\n",
            "2.3823415637016296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8sELCfvczCu"
      },
      "source": [
        "#### all_scores를 보면 2.1에서 2.7까지 다른 validation score가 나온것을 알 수 있다.\n",
        "#### 평균은 약 2.4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiNX6SGgczCu",
        "outputId": "fee0b9ec-bf21-4b73-d843-f488cb1bebb5"
      },
      "source": [
        "# Saving the validation logs at each fold\n",
        "num_epochs = 500\n",
        "all_mae_histories = []\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    val_data = train_data[i*num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples : (i + 1) * num_val_samples]\n",
        "    \n",
        "    partial_train_data = np.concatenate(\n",
        "    [train_data[:i * num_val_samples],\n",
        "    train_data[(i + 1) * num_val_samples :]],\n",
        "    axis = 0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "    [train_targets[:i * num_val_samples],\n",
        "    train_targets[(i + 1) * num_val_samples :]],\n",
        "    axis = 0)\n",
        "    \n",
        "    model = build_model()\n",
        "    history = model.fit(partial_train_data, partial_train_targets, validation_data = (val_data, val_targets), epochs = num_epochs, batch_size = 1, verbose = 0)\n",
        "    mae_history = history.history['val_mean_absolute_error']\n",
        "    all_mae_histories.append(mae_history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'val_mean_absolute_error'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-16-debafef0d42c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial_train_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartial_train_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mmae_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_mean_absolute_error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mall_mae_histories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmae_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'val_mean_absolute_error'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpJLobdKczCv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}